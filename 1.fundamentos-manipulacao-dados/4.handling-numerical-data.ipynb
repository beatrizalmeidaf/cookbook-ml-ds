{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Numerical Data\n",
    "\n",
    "---\n",
    "\n",
    "## Diferença entre Normalização e Padronização\n",
    "- **Min-Max Scaling (Normalização)** → Reescala os valores para um intervalo fixo (exemplo: 0 a 1).\n",
    "- **StandardScaler (Padronização)** → Ajusta os valores para terem **média 0 e desvio padrão 1**.\n",
    "\n",
    "Se os dados seguem uma distribuição normal (ou aproximadamente normal), o StandardScaler é a melhor opção, sendo ideal para modelos que assumem essa distribuição, como PCA e Regressão Linear. Já o MinMaxScaler é útil quando o modelo se beneficia de valores em um intervalo fixo, como redes neurais e KNN, pois normaliza os dados para um range específico, preservando a escala original sem alterar a forma da distribuição.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é o Min-Max Scaling?\n",
    "O **Min-Max Scaling** é uma técnica de normalização usada em Machine Learning para **reescalar os valores de uma variável para um intervalo específico**, geralmente entre **0 e 1** ou **-1 e 1**.\n",
    "\n",
    "Ele ajusta os valores de uma feature de forma que **o menor valor se torne 0 e o maior valor se torne 1**, mantendo a mesma distribuição relativa dos dados.\n",
    "\n",
    "---\n",
    "\n",
    "### Por que usar Min-Max Scaling?\n",
    "Muitos algoritmos de Machine Learning, como **Redes Neurais, K-Means e SVM**, funcionam melhor quando os dados estão na **mesma escala**.  \n",
    "Se os valores das variáveis tiverem faixas muito diferentes (por exemplo, uma feature variando de 0 a 1 e outra de 1.000 a 10.000), o modelo pode **dar mais peso para as features com valores maiores**, impactando negativamente no desempenho.\n",
    "\n",
    "### Benefícios do Min-Max Scaling:\n",
    "- **Evita que valores grandes dominem o modelo**.\n",
    "- **Melhora a estabilidade numérica de algoritmos** baseados em gradiente (como Redes Neurais).\n",
    "- **Ajuda modelos de distância (como KNN e K-Means)** a fazer comparações mais justas entre as variáveis.\n",
    "\n",
    "\n",
    "O Min-Max Scaler é essencial para **evitar problemas de escala entre variáveis**, garantindo que o modelo de Machine Learning **aprenda de maneira equilibrada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing \n",
    "\n",
    "feature = np.array([[-500.5],\n",
    "                    [-100.1],\n",
    "                    [0],\n",
    "                    [100.1],\n",
    "                    [900.9]])\n",
    "\n",
    "feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.28571429],\n",
       "       [0.35714286],\n",
       "       [0.42857143],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1)) # intervalo\n",
    "scaled_feature = minmax_scale.fit_transform(feature)\n",
    "\n",
    "scaled_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é a Padronização (Standardization)?\n",
    "A **padronização** é uma técnica de transformação de dados usada para ajustar uma feature para ter **média 0 e desvio padrão 1**. Isso garante que os valores sejam comparáveis em termos de quantos desvios padrão estão afastados da média.\n",
    "\n",
    "---\n",
    "\n",
    "### Por que usar StandardScaler?\n",
    "Diferente da normalização (**Min-Max Scaling**), que reescala os valores para um intervalo fixo, a **padronização transforma os dados em uma distribuição com média zero**. Isso é essencial para **modelos baseados em distribuições estatísticas**.\n",
    "\n",
    "### Benefícios da padronização:\n",
    "- **Torna os dados comparáveis**, removendo viés de escala.\n",
    "- **Evita que valores extremos tenham impacto desproporcional**.\n",
    "- **É amplamente usada em modelos estatísticos e Machine Learning**, como:\n",
    "  - **Regressão Linear**\n",
    "  - **PCA (Análise de Componentes Principais)**\n",
    "  - **SVM (Support Vector Machine)**\n",
    "  - **K-Means**\n",
    "  - **Redes Neurais (às vezes, dependendo do caso)**\n",
    "\n",
    "O **StandardScaler** é essencial para tornar os dados comparáveis, removendo influência da escala e garantindo que os modelos estatísticos **aprendam padrões reais, não apenas diferenças de magnitude**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76058269],\n",
       "       [-0.54177196],\n",
       "       [-0.35009716],\n",
       "       [-0.32271504],\n",
       "       [ 1.97516685]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[-1000.1],\n",
    "              [-200.2],\n",
    "              [500.5],\n",
    "              [600.6],\n",
    "              [9000.9]])\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "standardized = scaler.fit_transform(x)\n",
    "\n",
    "standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0\n",
      "Standard deviation: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean:\", round(standardized.mean()))\n",
    "print(\"Standard deviation:\", standardized.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é a Normalização de Observações?\n",
    "A **normalização** (normalization) é uma técnica que **reescala** os valores de uma observação (linha) para que **tenham um comprimento total de 1**. Isso é feito dividindo cada valor da observação por uma norma específica.\n",
    "\n",
    "Ao contrário de técnicas como MinMaxScaler e StandardScaler, que atuam **nas features (colunas)**, a normalização atua **nas observações (linhas)**.\n",
    "\n",
    "---\n",
    "\n",
    "### Por que usar Normalizer?\n",
    "- **Torna os dados comparáveis em magnitude** independentemente da escala original.\n",
    "- **Útil para dados esparsos ou de alta dimensionalidade**, como:\n",
    "  - **Text Mining (TF-IDF, Bag of Words)**\n",
    "  - **Vetores de características em Machine Learning**\n",
    "  - **Redes Neurais, onde vetores normalizados podem melhorar a estabilidade do treinamento**\n",
    "\n",
    "### Benefícios:\n",
    "- Mantém a **proporção relativa entre os valores** dentro de cada linha.\n",
    "- **Ajuda a evitar que valores grandes dominem o modelo**.\n",
    "- **Útil para algoritmos baseados em similaridade vetorial**, como:\n",
    "  - **KNN (K-Nearest Neighbors)**\n",
    "  - **SVM (Support Vector Machine)**\n",
    "  - **Redes Neurais**\n",
    "  - **Modelos baseados em distâncias (ex: Cosine Similarity)**\n",
    "\n",
    "### **Diferença entre L1 e L2**\n",
    "- **L2 (Euclidiana):** Vetores de magnitude 1 e é útil quando a relação angular entre os pontos é mais importante do que a escala dos valores individuais.\n",
    "- **L1 (Manhattan):** Preserva esparsidade dos dados (mantém zeros intactos), soma total sempre é 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.70710678],\n",
       "       [0.30782029, 0.95144452],\n",
       "       [0.07405353, 0.99725427],\n",
       "       [0.04733062, 0.99887928],\n",
       "       [0.95709822, 0.28976368]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "features = np.array([[0.5, 0.5],\n",
    "                    [1.1, 3.4],\n",
    "                    [1.5, 20.2],\n",
    "                    [1.63, 34.4],\n",
    "                    [10.9, 3.3]])\n",
    "\n",
    "normalizer = Normalizer(norm=\"l2\")\n",
    "\n",
    "normalizer.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformação de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_array = np.array([[2, 3],\n",
    "                          [2, 3],\n",
    "                          [2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 13],\n",
       "       [12, 13],\n",
       "       [12, 13]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_ten(x):\n",
    "    return x + 10\n",
    "\n",
    "ten_transformer = FunctionTransformer(add_ten)\n",
    "\n",
    "feature_array = ten_transformer.transform(feature_array)\n",
    "feature_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretizando Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "age = np.array([[6],\n",
    "                [12],\n",
    "                [20],\n",
    "                [36],\n",
    "                [65]])\n",
    "\n",
    "binarizer = Binarizer(threshold=18) # limiar\n",
    "\n",
    "age = binarizer.fit_transform(age)\n",
    "age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupando Observações com Clusterização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9.877554</td>\n",
       "      <td>-3.336145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7.287210</td>\n",
       "      <td>-8.353986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-6.943061</td>\n",
       "      <td>-7.023744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7.440167</td>\n",
       "      <td>-8.791959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6.641388</td>\n",
       "      <td>-8.075888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2\n",
       "0  -9.877554  -3.336145\n",
       "1  -7.287210  -8.353986\n",
       "2  -6.943061  -7.023744\n",
       "3  -7.440167  -8.791959\n",
       "4  -6.641388  -8.075888"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "features, _ = make_blobs(n_samples=50, n_features=2, centers=3, random_state=1)\n",
    "\n",
    "dataframe = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BeatrizAlmeida\\anaconda3\\envs\\estatistica\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9.877554</td>\n",
       "      <td>-3.336145</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7.287210</td>\n",
       "      <td>-8.353986</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-6.943061</td>\n",
       "      <td>-7.023744</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7.440167</td>\n",
       "      <td>-8.791959</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6.641388</td>\n",
       "      <td>-8.075888</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  group\n",
       "0  -9.877554  -3.336145      2\n",
       "1  -7.287210  -8.353986      0\n",
       "2  -6.943061  -7.023744      0\n",
       "3  -7.440167  -8.791959      0\n",
       "4  -6.641388  -8.075888      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = KMeans(3, random_state=0)\n",
    "clusters.fit(features)\n",
    "\n",
    "dataframe[\"group\"] = clusters.predict(features)\n",
    "dataframe.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "estatistica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
