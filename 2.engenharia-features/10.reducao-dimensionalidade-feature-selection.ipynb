{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction Using Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleção de Features: Uma Abordagem para Redução de Dimensionalidade  \n",
    "\n",
    "A **seleção de features** consiste em escolher apenas as features mais relevantes e descartar as menos úteis.  \n",
    "\n",
    "### Tipos de Seleção de Features  \n",
    "\n",
    "Existem **três métodos principais** para selecionar features:  \n",
    "\n",
    "1. **Métodos de Filtro**  \n",
    "   - Escolhem as melhores features analisando suas propriedades estatísticas.  \n",
    "\n",
    "2. **Métodos Wrapper**  \n",
    "   - Testam diferentes combinações de features para encontrar o conjunto que gera o melhor desempenho no modelo.  \n",
    "\n",
    "3. **Métodos Embutidos (Embedded)**  \n",
    "   - A seleção de features acontece como parte do treinamento de um algoritmo de aprendizado de máquina.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Variance Thresholding\n",
    "\n",
    "Remove colunas (features) que tem variação muito baixa, ou seja, imagina que tem um monte de colunas em uma tabela cheia de números e quer tirar as que quase não mudam, então remove as com variância baixa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso, calculamos a variância de cada coluna, se os valores de uma coluna mudam bastante então a variância é alta. Depois definimos um limite e removemos as colunas com a variância menor que esse limite.\n",
    "\n",
    "Aviso: não funciona bem se os dados estiverem em escalas diferentes, porém se padronizar tudo também não será uma técnica útil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 1.4, 0.2],\n",
       "       [4.9, 1.4, 0.2],\n",
       "       [4.7, 1.3, 0.2]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "thresholder = VarianceThreshold(threshold=.5) # variância menor que 0.5\n",
    "\n",
    "features_high_variance = thresholder.fit_transform(features) # aplicamos o filtro que remove as colunas com variância baixa\n",
    "\n",
    "features_high_variance[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68112222, 0.18871289, 3.09550267, 0.57713289])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholder.fit(features).variances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Thresholding Binary Feature Variance\n",
    "\n",
    "Técnica usada quando temos um monte de colunas com valores binários (0 ou 1) e queremos remover aquelas que quase não mudam.\n",
    "\n",
    "Deve ser usado quando:\n",
    "- Se tem muitas colunas binárias (0s e 1s) e quer remover as que são quase sempre iguais.\n",
    "- Se os dados têm muitas features desbalanceadas (exemplo: uma coluna que tem 99% de 0s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "features = [[0, 1, 0],\n",
    "            [0, 1, 1],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 1],\n",
    "            [1, 0, 0]]\n",
    "\n",
    "thresholder = VarianceThreshold(threshold=(0.75 * (1 - 0.75)))\n",
    "\n",
    "features_filtradas = thresholder.fit_transform(features)\n",
    "features_filtradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handling Highly Correlated Features\n",
    "\n",
    "Quando tem um conjunto de dados com várias colunas (features) e suspeita que algumas delas sejam muito parecidas (ou seja, são altamente correlacionadas). Por isso é necessário descobrir quais colunas são muito parecidas e remover uma delas.\n",
    "\n",
    "Para isso:\n",
    "- Criamos uma matriz de correlação\n",
    "- Procura correlações muito altas\n",
    "- Remove uma das colunas correlacionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  1  1  1\n",
       "1  2  2  0\n",
       "2  3  3  1\n",
       "3  4  4  0\n",
       "4  5  5  1\n",
       "5  6  6  0\n",
       "6  7  7  1\n",
       "7  8  7  0\n",
       "8  9  7  1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "features = np.array([[1, 1, 1],\n",
    "                     [2, 2, 0],\n",
    "                     [3, 3, 1],\n",
    "                     [4, 4, 0],\n",
    "                     [5, 5, 1],\n",
    "                     [6, 6, 0],\n",
    "                     [7, 7, 1],\n",
    "                     [8, 7, 0],\n",
    "                     [9, 7, 1]])\n",
    "\n",
    "df = pd.DataFrame(features)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  2\n",
       "0  1  1\n",
       "1  2  0\n",
       "2  3  1\n",
       "3  4  0\n",
       "4  5  1\n",
       "5  6  0\n",
       "6  7  1\n",
       "7  8  0\n",
       "8  9  1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matriz = df.corr().abs() # pega os valores absolutos \n",
    "\n",
    "# usar somente a parte superior da matriz para não repetir os pares\n",
    "upper = corr_matriz.where(np.triu(np.ones(corr_matriz.shape), k=1).astype(bool))\n",
    "\n",
    "# encontramos as colunas com correlação maior que 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "df_filtrado = df.drop(df.columns[to_drop], axis=1)\n",
    "df_filtrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Removing Irrelevant Features for Classification\n",
    "\n",
    "Usamos quando temos um conjunto de dados com várias colunas (features) e um alvo (target) que queremos prever. Mas algumas dessas colunas podem ser irrelevantes para a previsão. Por isso escolhemos as mais úteis usando testes estatísticos que medem a relação entre as colunas e o target (veêm quais tem mais relações com a target).\n",
    "\n",
    "- Qui-quadrado (χ²) → Para colunas categóricas\n",
    "- F-Valor (ANOVA F-test) → Para colunas numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número original de colunas: 4\n",
      "Número reduzido de colunas: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# transforma dados para inteiros pois chi² precisa de valores positivos\n",
    "features = features.astype(int)\n",
    "\n",
    "# selecionamos as 2 colunas mais relevantes com chi²\n",
    "chi2_selector = SelectKBest(chi2, k=2)\n",
    "features_kbest = chi2_selector.fit_transform(features, target)\n",
    "\n",
    "print(\"Número original de colunas:\", features.shape[1])\n",
    "print(\"Número reduzido de colunas:\", features_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número original de colunas: 4\n",
      "Número reduzido de colunas: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# selecionamos as 2 colunas mais relevantes usando ANOVA F-test\n",
    "fvalue_selector = SelectKBest(f_classif, k=2)\n",
    "features_kbest = fvalue_selector.fit_transform(features, target)\n",
    "\n",
    "print(\"Número original de colunas:\", features.shape[1])\n",
    "print(\"Número reduzido de colunas:\", features_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se não sabe quantas colunas manter → Em vez de SelectKBest(k=2), pode usar SelectPercentile(percentile=75), que mantém as melhores 75% das colunas\n",
    "\n",
    "**Por fim, se uma coluna não ajuda a prever o target, ela é removida para deixar o modelo mais eficiente**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recursively Eliminating Features\n",
    "\n",
    "O RFE escolhe as features mais importantes para o modelo automaticamente. Ele usa validação cruzada para garantir que a remoção de variáveis não degrade a performance. Quanto menor o número de variáveis relevantes, mais eficiente fica o modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00850799,  0.0992611 ,  0.7031277 , -0.75164317, -1.20934598],\n",
       "       [-1.07500204,  0.92859616,  2.56148527, -1.68392493, -1.73223805],\n",
       "       [ 1.37940721,  1.83471056, -1.77039484,  0.12954322,  0.03519776],\n",
       "       ...,\n",
       "       [-0.80331656, -0.40335314, -1.60648007,  0.20986659, -0.05626806],\n",
       "       [ 0.39508844, -0.98395086, -1.34564911, -1.21942363, -1.50212012],\n",
       "       [-0.55383035,  0.05065251,  0.82880112,  0.05486622,  0.64327968]],\n",
       "      shape=(10000, 5))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "\n",
    "features, target = make_regression(n_samples = 10000, n_features = 100, n_informative = 2, random_state = 1)\n",
    "\n",
    "ols = linear_model.LinearRegression()\n",
    "\n",
    "rfecv = RFECV(estimator=ols, step=1, scoring=\"neg_mean_squared_error\")\n",
    "rfecv.fit(features, target)\n",
    "rfecv.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# número de melhores features\n",
    "rfecv.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([67, 76, 71, 52, 69,  1, 95, 94, 78, 64, 11, 75, 70, 90, 14, 19, 20,\n",
       "       77, 83,  1, 59, 21, 12,  9, 13, 39, 22, 10,  8, 93, 89, 62, 55, 73,\n",
       "       45, 88, 72, 79, 26,  1,  1, 16,  6, 42, 28, 37,  2, 63, 96, 46, 86,\n",
       "       32, 54, 65, 68, 91, 24, 57,  1, 84, 49, 53, 50, 60, 23, 87, 27, 30,\n",
       "       36,  5, 29, 51, 17, 18, 34, 31,  3, 58, 43,  7, 44, 66, 80, 33, 15,\n",
       "       85, 40, 61, 38, 56, 92, 74, 81, 35,  4, 25, 82, 48, 41, 47])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfecv.ranking_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "estatistica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
